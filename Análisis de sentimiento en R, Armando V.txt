# Análisis de Sentimiento en R sobre el NAFTA
#Armando Valdés López

#si ya se tienen instaladas, no instalar
# install.packages ("twitteR")
# install.packages ("RCurl")
# install.packages ("tm")
# install.packages ("wordcloud")
# install.packages ("stopwords")
# install.packages ("stringi")
# install.packages("sentimentr")
# install.packages("stringr")

# _____________________________
# install.packages ("pattern.nlp")
# install.packages ("mscstexta4r")
# install.packages ("plyr")
# ________________________________


# se pueden pedir con library o así
require (twitteR)
require (RCurl)
require(tm)
require(wordcloud)
require(stopwords)
require(stringi)
require(sentimentr)
require(stringr)


# require(mscstexta4r)
# require(plyr)



# Aquí van los token que cada uno genere en twitter
consumer_key <- '7IPHuZwhjS562JlwcIq7NAoQ1'
consumer_secret <- 'HpOmDVpBU2jiiDHy9jLmvwyCVzQMG4eSkrlv5sVmzEgQzco3V1'
access_token <- '983924966021361664-LO4wu1KiWJLOPmg5Tw47Vgaqa1tIzs9'
acces_secret <- '68yB1hs4J6r5K5VK4Vn5eKTDqdSMqBr5yc2JNeZgyGrk8'
setup_twitter_oauth(consumer_key,consumer_secret,access_token,acces_secret)


#buscamos como ejemplo la palabra "NAFTA"
# resultType="recent"   most recent tweets
# resultType="popular"   most popular tweets
# resultType="mixed"    most popular tweets plus real time tweets

NAFTA_tweets <- searchTwitter("NAFTA", n=100, lang="en", resultType="mixed")
# NAFTA_tweets
str(NAFTA_tweets)
NAFTA_tweets [1:3]
class(NAFTA_tweets)


# Convertimos la lista en un vector
NAFTA_text <- sapply(NAFTA_tweets, function(x) x$getText())
str(NAFTA_text)



# Creamos el corpus

NAFTA_corpus <- stri_trans_general(NAFTA_text, "latin-ascii")

# removemos emojis
NAFTA_corpus <- iconv(NAFTA_corpus, "ASCII", "UTF-8", sub="byte")


NAFTA_corpus <- Corpus(VectorSource(NAFTA_corpus))
NAFTA_corpus
inspect(NAFTA_corpus[1])

# gsub(pattern="ã¡", "a", NAFTA_corpus)
# gsub(pattern="ã©", "e", NAFTA_corpus)
# gsub(pattern="ã¨", "e", NAFTA_corpus)
# gsub(pattern="ã", "i", NAFTA_corpus)
# gsub(pattern="ã²", "o", NAFTA_corpus)
# gsub(pattern="ã³", "o", NAFTA_corpus)
# gsub(pattern="ãº", "u", NAFTA_corpus)
# gsub(pattern="ã±", "ñ", NAFTA_corpus)
# gsub(pattern="\\W", " ", NAFTA_corpus)
# gsub(pattern="\\b[A-z]\\b{1}", replace=" ", NAFTA_corpus)



# herramientas que vimos en clase para remover puntuaciones, numeros etc
NAFTA_clean <- tm_map(NAFTA_corpus, removePunctuation)
NAFTA_clean <- tm_map(NAFTA_clean, content_transformer(stri_trans_tolower))
NAFTA_clean <- tm_map(NAFTA_clean, removeWords, stopwords("english"))
NAFTA_clean <- tm_map(NAFTA_clean, removeNumbers)
NAFTA_clean <- tm_map(NAFTA_clean, stripWhitespace)
NAFTA_clean <- tm_map(NAFTA_clean, removeWords, c("NAFTA", "nafta"))


# Creating a wordcloud graphic
wordcloud(NAFTA_clean, random.order=F, col=rainbow(50))
writeCorpus(NAFTA_clean)
writeLines(as.character(NAFTA_clean), con="NAFTA_clean2.txt")


# Análisis de sentimiento
file.choose()
# buscamos el documento en nuestro directorio
folder <- "C:\\Users\\valde\\Documents\\Diplomado tec"
list.files(path=folder)
list.files(path=folder, pattern="*.txt")
filelist <- list.files(path=folder, pattern="NAFTA_clean2.txt")
filelist
typeof(filelist)
lapply(filelist, FUN=readLines)
corpus2 <- lapply(filelist, FUN=readLines)
lapply (corpus2, FUN=paste, collapse="")
corpus2 <- lapply (corpus2, FUN=paste, collapse="")


# limpiamos el documento
gsub(patter="\\W", replace=" ", corpus2)
corpus3 <- gsub(patter="\\W", replace=" ", corpus2)
gsub(pattern="\\d", replace=" ", corpus3)
corpus3 <- gsub(pattern="\\d", replace=" ", corpus3)
tolower(corpus3)
corpus3 <- tolower(corpus3)
removeWords(corpus3, stopwords())
corpus3 <- removeWords(corpus3, stopwords())
gsub(pattern="\\b[A-z]\\b{1}", replace=" ", corpus3)
corpus3 <- gsub(pattern="\\b[A-z]\\b{1}", replace=" ", corpus3)
stripWhitespace(corpus3)
corpus3 <- stripWhitespace(corpus3)
x11()
wordcloud(corpus3, random.order=F, col=rainbow(50))

#buscamos los diccionarios de palabras positivas y negativas
opinion.lexicon.pos <- scan('positive-words.txt', what='character', comment.char=';')
opinion.lexicon.neg <- scan('negative-words.txt', what='character', comment.char=';')
str_split(corpus3, pattern="\\s+")
NAFTA_bag <- str_split(corpus3,pattern="\\s+")
class(NAFTA_bag)
NAFTA_bag
lapply(NAFTA_bag, function(x) {sum(!is.na(match(x,opinion.lexicon.pos)))})
lapply(NAFTA_bag, function(x) {sum(!is.na(match(x,opinion.lexicon.neg)))})


# ranking de sentimiento
# sólo hace un balance de negativas menos positivas
lapply(NAFTA_bag, function(x) {sum(!is.na(match(x,opinion.lexicon.pos))) - sum(!is.na(match(x,opinion.lexicon.neg)))})
score <- lapply(NAFTA_bag, function(x) {sum(!is.na(match(x,opinion.lexicon.pos))) - sum(!is.na(match(x,opinion.lexicon.neg)))})
score


# Para múltiples documentos
# mean(score)
# sd(score)
# hist(score)






# ___________________________________
# Otro ejemplo
# AMLO_tweets <- searchTwitter("AMLO", n=100, lang="es", resultType="recent")
# AMLO_tweets
# str(AMLO_tweets)